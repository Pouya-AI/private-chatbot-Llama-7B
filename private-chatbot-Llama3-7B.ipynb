{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:51:23.159889Z",
     "iopub.status.busy": "2024-07-10T20:51:23.159220Z",
     "iopub.status.idle": "2024-07-10T20:51:25.418643Z",
     "shell.execute_reply": "2024-07-10T20:51:25.417656Z",
     "shell.execute_reply.started": "2024-07-10T20:51:23.159859Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:51:25.421392Z",
     "iopub.status.busy": "2024-07-10T20:51:25.420518Z",
     "iopub.status.idle": "2024-07-10T20:51:32.507880Z",
     "shell.execute_reply": "2024-07-10T20:51:32.507058Z",
     "shell.execute_reply.started": "2024-07-10T20:51:25.421357Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc0a99d455b440c867fc32c1f28c572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "access_token = 'fdgOIYFUJHGfihgfduyiJHGigHLGFyfHgyDFG' # do not test it this is the fake one it will not work :))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",use_auth_token=access_token,\n",
    "                                        trust_remote_code=True,)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",use_auth_token=access_token,\n",
    "                                        trust_remote_code=True,device_map='cuda',torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:51:40.227192Z",
     "iopub.status.busy": "2024-07-10T20:51:40.226777Z",
     "iopub.status.idle": "2024-07-10T20:51:40.231987Z",
     "shell.execute_reply": "2024-07-10T20:51:40.230956Z",
     "shell.execute_reply.started": "2024-07-10T20:51:40.227158Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:51:40.510816Z",
     "iopub.status.busy": "2024-07-10T20:51:40.510128Z",
     "iopub.status.idle": "2024-07-10T20:51:40.520684Z",
     "shell.execute_reply": "2024-07-10T20:51:40.519805Z",
     "shell.execute_reply.started": "2024-07-10T20:51:40.510780Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self,model,tokenizer,system_content=None):\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "        torch.backends.cuda.enable_flash_sdp(False)\n",
    "        \n",
    "        self.terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        system_content = system_content if system_content else \"You are a helpful assistant who answers short.\"  \n",
    "        \n",
    "        message =  [{\"role\": \"system\", \"content\": system_content}]\n",
    "        self.input_ids = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "        \n",
    "    def response(self,user_content):\n",
    "        message = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        new_input_ids = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "        self.input_ids = torch.cat((self.input_ids,new_input_ids),dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                self.input_ids,\n",
    "                max_new_tokens=256,\n",
    "                eos_token_id=self.terminators,\n",
    "                pad_token_id=128009,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                top_p=0.5,\n",
    "            )\n",
    "        \n",
    "        response = outputs[0][self.input_ids.shape[-1]:]\n",
    "        print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "        \n",
    "        self.input_ids = outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:51:42.642801Z",
     "iopub.status.busy": "2024-07-10T20:51:42.641917Z",
     "iopub.status.idle": "2024-07-10T20:51:42.681743Z",
     "shell.execute_reply": "2024-07-10T20:51:42.680960Z",
     "shell.execute_reply.started": "2024-07-10T20:51:42.642765Z"
    }
   },
   "outputs": [],
   "source": [
    "chatbot = ChatBot(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:51:42.899309Z",
     "iopub.status.busy": "2024-07-10T20:51:42.898828Z",
     "iopub.status.idle": "2024-07-10T20:51:42.904816Z",
     "shell.execute_reply": "2024-07-10T20:51:42.903905Z",
     "shell.execute_reply.started": "2024-07-10T20:51:42.899280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant who answers short.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(chatbot.input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:52:24.586133Z",
     "iopub.status.busy": "2024-07-10T20:52:24.585725Z",
     "iopub.status.idle": "2024-07-10T20:54:01.468895Z",
     "shell.execute_reply": "2024-07-10T20:54:01.467798Z",
     "shell.execute_reply.started": "2024-07-10T20:52:24.586096Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: Hi! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Do you know about LLMs?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: You're referring to Large Language Models! Yes, I'm familiar with them. LLMs are a type of artificial intelligence designed to process and generate human-like language. They're trained on vast amounts of text data and can perform tasks such as language translation, text summarization, and even creative writing. Would you like to know more about them?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What are the most common LLMs nowadays?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: Some of the most well-known and widely used Large Language Models (LLMs) nowadays include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT is a popular LLM that's been fine-tuned for various NLP tasks.\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach): Another Google-developed LLM, RoBERTa is a variant of BERT that's known for its improved performance on many NLP tasks.\n",
      "3. XLNet: Developed by Google and Carnegie Mellon University, XLNet is a state-of-the-art LLM that uses a novel permutation-based language modeling approach.\n",
      "4. T5 (Text-to-Text Transformer): Developed by Google, T5 is a general-purpose LLM that can be fine-tuned for various NLP tasks, including language translation, text summarization, and more.\n",
      "5. DALL-E: Developed by Meta AI, DALL-E is a text-to-image LLM that can generate high-quality images from text prompts.\n",
      "\n",
      "These are just a few examples, but there are many more LLMs being developed and fine-tuned for specific tasks and applications.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Thanks for your service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: You're welcome! It was my pleasure to help. If you have any more questions or need assistance with anything else, feel free to ask!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Exit\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    text = input('\\nUser:')\n",
    "    if text == 'Q' or text == 'Exit':\n",
    "        break\n",
    "    print('\\nBot: ',end='')\n",
    "    chatbot.response(text)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
